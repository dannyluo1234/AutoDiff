{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ee9db91-4270-4851-ae2e-273e4287bafe",
   "metadata": {},
   "source": [
    "# cs107 - FinalProject"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05caa320-56f8-4799-a25e-82aa1bfcdbdd",
   "metadata": {},
   "source": [
    "## Group Number: 17 \n",
    "### Group Members:\n",
    "- Diana Feng\n",
    "- Varshini Subhash\n",
    "- Xiaobo Luo\n",
    "- Adriana Trejo-Sheu\n",
    "\n",
    "Description: This is the final project repo for Group 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c638c3-93b0-4c73-a53a-ecc424565488",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "​\tDifferentiation is a giant part of mathematics, engineering, and computer science. It is important to have a systematic way to solve complex differentiation problems through the use of software instead of doing it by hand, which our Automatic Differentiation package works to accomplish. Automatic Differentiation (AD) is a series of computational techniques to evaluate the derivatives of functions that are specified for computers. AD can be completed either using the fast-forward method, which we will be implementing, or the reverse method. The overall goal of AD is to numerically differentiate a function through the use of point values and formulas, in order to get a numerical value that does not have any approximation error. The fast forward method divides a complex function into a series of elementary derivatives that eventually build up to the final function.\n",
    "​\tOur library will provide clients the ability to run fast-forward Automatic differentiation as well as another expansion. This package contains all the necessary dependencies that a client will need as well as any documentation to help correctly utilize the program. It also contains mathematical background information in case any clients are not familiar with calculations that are used in AD. This package will be available through PyPI and can be easily downloaded through the website pypi.org. It is currently not up on PyPI but can be downloaded from the github and that is walked through in a later section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9920a3fa-3935-48ca-a8b0-f5927543175d",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "​\tAutomatic differentiation relies on the fundamental building block of calculus, which is the chain rule. A derivative essentially captures the rate of change of a function with respect to a variable and the chain rule states that for three variables x, y and z, knowing the rate of change of x with respect to y $(\\frac{dx}{dy})$ and y with respect to z $(\\frac{dy}{dz})$ allows us to calculate the rate of change of x with respect to z as the product of the two rates of changes $(\\frac{dx}{dy})$ and $(\\frac{dy}{dz})$. \n",
    "In other words, the chain rule is:\n",
    "$$(\\frac{dx}{dz}) = (\\frac{dx}{dy})(\\frac{dy}{dz}) $$\n",
    "\n",
    "We can generalize this to an arbitrary number of variables and dimensions with the gradient operator. Here is an example of a gradient of a function.\n",
    "$$ F = x^2 + 3y^3 $$\n",
    "\n",
    "$$\\nabla F = \\begin{bmatrix} \n",
    "\\frac{\\partial{F}}{\\partial{x}}\\\\\n",
    "\\frac{\\partial{F}}{\\partial{y}}\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "​\tThere is a certain ordering associated with the chain rule, where the derivative of the outermost function can be obtained if you evaluate the derivatives of the nested functions from inside out. These intermediate results can be stored as nodes of a graph, which we call the computational graph. For instance, the input to the innermost function can be considered as x and placed at the left end of the graph and the ordering of differentiation operations (as part of the chain rule) can be visualised as the order of the nodes following x, until we reach the final node on the right which will be the final derivative we are seeking.\n",
    "\n",
    "​\t This constructs the graph in the forward direction from the input x to the derivative of our function f. The reverse mode encompasses the construction of this graph in the opposite direction. The intermediate results we obtain are defined as dependent variables while the original coordinates we begin with (in our case x, which is a vector) are called independent variables. This is because we differentiate with respect to our independent variables, to find the rate of change of the dependent variables.Thus, we can conclude that the forward mode computes the gradient of the function f with respect to the independent variable by traversing forward and the reverse mode computes the sensitivity of the function f with respect to both independent and dependent variables by traversing backward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93c5a86-0e21-4f87-afed-94b849ba4a14",
   "metadata": {},
   "source": [
    "## How to use cs107-creativename\n",
    "\n",
    "​\tThe user will use `pip` to install the package. When users install the package, its dependencies will automatically be added as well so that the only thing that the user needs to import is our library (similar to how numpy is a dependency of pandas, but users only need to `import pandas` to use all of its functionality)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46053a43-e0e1-4c69-80c2-401cdb88766b",
   "metadata": {},
   "source": [
    "Steps to download: \n",
    "You want to download the code from our repo to your local computer. \n",
    "\n",
    "`git clone https://github.com/cs107-creativename/cs107-FinalProject.git`\n",
    "While in the folder to download everything locally. \\\n",
    "`pip install .`\n",
    "\n",
    "The other way to install is to download our package from TestPyPI. \n",
    "`pip install -i https://test.pypi.org/simple/ cs107-creativename`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d107af6-05c0-412b-9336-2843a39eca5d",
   "metadata": {},
   "source": [
    "​\tTo instantiate an AD object, the user will just call the AD function within the library with the function to be differentiated. By passing in additional optional methods, the user can customize the AD process. After creating the differentiated function, the user can then call this function with the value to be evaluated in the form of a Variable class within our library. The user is also able to call the gradient of the function. To try a couple cases follow the interactive case below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc35e41-84b6-41fc-82a1-659e3084b036",
   "metadata": {},
   "source": [
    "#### Importing the Functions\n",
    "**Note** You must import Functions in order to have any trig or additional functions be used. This can also be taken care of if you run the code: \\\n",
    "`from cs107_creativename import *`\n",
    "**Note** In order to have this Juptyer notebook work you must have our package downloaded which can be done be either uncommenting the next cell to have it imported locally or using the pip install method with TestPyPI. By installing locally, you are able to run tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f36d050-94f9-4d2e-81a8-299078059fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install ../."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3959111-e003-4b32-925d-e2ae46c8e944",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cs107_creativename.Forward import *\n",
    "from cs107_creativename.Functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3b5ab2-a520-4dc4-81d8-34e0534d8641",
   "metadata": {},
   "source": [
    "#### Instantiate an object\n",
    "This will make the x variable a Forward.Dual class variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8887c5f9-7a0e-420e-b313-c5f38a40fc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'cs107_creativename.Forward.Dual'>\n"
     ]
    }
   ],
   "source": [
    "x = Dual(3)\n",
    "print(type(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20385f03-9eae-46ea-bfda-ea4cce4b04df",
   "metadata": {},
   "source": [
    "#### Using the Forward class with one dimension (i.e. having one variable x)\n",
    "Once we instantiate x, we can use it in function that we can then store the value of the function and the derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99c7368b-20e8-4bf9-98bf-f6605ac67895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8414709848078967 3.5403023058681398\n"
     ]
    }
   ],
   "source": [
    "# Single dimension input case, x=1\n",
    "x = Dual(1)\n",
    "f = sin(x) + x + x**2\n",
    "print(f.val, f.der)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba06e49-7e3d-4593-bfd9-37dd6648131c",
   "metadata": {},
   "source": [
    "#### Using the Forward class with Two dimensions (i.e having two variables x and y)\n",
    "The previous example can be expanded to instantiate multiple variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2701162e-e01a-4f34-951b-3a0fe022b0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.09070257317431829 -1.2484405096414273\n"
     ]
    }
   ],
   "source": [
    "x = Dual(1)\n",
    "y = Dual(2)\n",
    "f = sin(x*y) + x**y -2*x\n",
    "print(f.val, f.der)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dc62ab-e141-42f3-8315-541b7524a083",
   "metadata": {},
   "source": [
    "#### Calculate derivative in a specific direction (gradient dot product with the direction)\n",
    "Same set up as above, but now we are changing the direction (2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "400bae47-d129-44a1-8fa7-578d7fca14fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.09070257317431829 -2.9130278558299967\n"
     ]
    }
   ],
   "source": [
    "x = Dual(1,2)\n",
    "y = Dual(2,3)\n",
    "f = sin(x*y) + x**y -2*x\n",
    "print(f.val, f.der)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd57feaf-e541-4bf7-a7ed-f26bc54496c6",
   "metadata": {},
   "source": [
    "#### Calculating gradient, note that the sin function is the ones we define\n",
    "How to fill out the AutoDiffF function: AutoDiffR(**function, values for variables in list format**).\n",
    "It returns the gradient, which includes the partial for each variable at the specified value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d48f1f1-456d-4f23-8cff-6d726c0818fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.5403023058681398, 27.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "def f(x, y, z):\n",
    "    return sin(x) + y**3 + x*z\n",
    "print(AutoDiffF(f,[1,3,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554ef3ee-b76c-4f90-8b6f-4d64304dd322",
   "metadata": {},
   "source": [
    "#### You can also use Lambda functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28fc4012-07e1-4913-953d-563f68defb46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.5403023058681398, 27.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "func = lambda x,y,z: sin(x) + y**3 + x*z\n",
    "print(AutoDiffF(func,[1,3,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f300a20-bad0-43c5-abc1-38b6f5233117",
   "metadata": {},
   "source": [
    "## Software Organization\n",
    "\n",
    "​\tThe overall structure will have multiple levels that all read from a main program that performs the AD process. There will be separate pathways that will solve different mathematical derivatives (i.e. separating derivatives of exponentials and trigonometric ones). We will include multiple packages that are loaded into the program that the client will have access to when downloading our library. The two main modules that we will have is a Dual class and our fast forward auto differentiation. The Dual class will be used to represent complex numbers that can be entered into our auto differentiation function. The auto differentiation function implements a forward mode auto differentiation by taking advantage of the chain rule. To test our program, you can run the test_suite.sh file that uses pytest and pytest-cov to run the tests and provide coverage of them. \n",
    "\n",
    "#### How to Run Tests:\n",
    "To run the test_suite file you can simply\n",
    "do: `bash test_suite.sh` or `./test_suite.sh`. \n",
    "\n",
    "We do not plan to use a framework to package our program. Here is what the structure of our program will look like:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf43ccb-8d45-4192-bd68-7b968324aa62",
   "metadata": {},
   "source": [
    "```\n",
    "|--main \n",
    "    |-- pyproject.toml\n",
    "    |--setup.cfg\n",
    "    |-- README.md \n",
    "\t|-- .gitignore\n",
    "\t|-- .coverage   \n",
    "\t|-- docs \n",
    "        |-- Documentation.ipynb\n",
    "\t\t|-- milestone1.md \n",
    "\t\t|-- milestone2.ipynb \n",
    "\t\t|-- milestone2_progress.md \n",
    "\t|-- License.md \n",
    "    |-- tests\n",
    "        |-- test_suite.sh\n",
    "        |-- cs107_creativename_tests\n",
    "            |-- tests.py\n",
    "\t|-- src \n",
    "\t\t|-- Functions.py\n",
    "\t\t|-- Reverse.py\n",
    "        |-- Forward.py\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4b43ad-1acf-4923-98b4-6092a85c5260",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "​\tThe core data structure is the dual number. It will have its own rule of addition, multiplication, etc. We will implement a class called “dual” to realize this dual data structure. We will also implement the main function for Forward mode which will utilize the dual class. The dual class will have functions like \\_\\_add__, \\_\\_mul__, \\_\\_sub__, \\_\\_truediv__, \\_\\_pow__ to overload the elementary operators. We will also have functions like, sin, cos which we use instead of the default math library functions. We will rely on functions and methods in packages, such as NumPy and math. We can use NumPy and math to evaluate the true value and value of the derivative for each node in our AD process. For example, we can use np.cos to evaluate a cosine function that is given as well as use it to evaluate the derivative if a sine function is give. Since there are no dunder methods designed for those math functions, we will write them in our dual class and ask the user to use our function instead of the ones in math or numpy. Within the dual class, we will implement functions; such as sin, sqrt, log, exp, etc. which will take in function arguments of type dual or float and return the corresponding evaluated value.\n",
    "\n",
    "Dual class functions covered: \n",
    "- \\_\\_add__\n",
    "- \\_\\_radd__\n",
    "- \\_\\_truediv__\n",
    "- \\_\\_rtruediv__\n",
    "- \\_\\_pow__ \n",
    "- \\_\\_rpow__ \n",
    "- \\_\\_mul__\n",
    "- \\_\\_rmul__\n",
    "- \\_\\_sub__\n",
    "- \\_\\_rsub__\n",
    "\n",
    "\n",
    "List of elementary functions covered: \n",
    "- sin()\n",
    "- cos()\n",
    "- tan()\n",
    "- arcsin()\n",
    "- arccos()\n",
    "- arctan()\n",
    "- sinh()\n",
    "- cosh()\n",
    "- tanh()\n",
    "- arcsinh()\n",
    "- arccosh()\n",
    "- arctan(h)\n",
    "- exp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bbe649-efe9-43c1-99f1-49cd4f278f9b",
   "metadata": {},
   "source": [
    "## License\n",
    "\n",
    "​\tFor the license of our project, we decided to go with the MIT License. We want the fruit of our labor to be able to help the work of others and potentially contribute to bigger and better projects, commercial or not. But in the meantime, we definitely do not want to be held liable for continuing the service of the software. We believe the MIT license allows us to do both."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc2b4e6-98fa-4bc5-8ccc-c75ac3682463",
   "metadata": {},
   "source": [
    "## Future Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbee9bb1-7c8d-445f-8ce8-78f61ad9215e",
   "metadata": {},
   "source": [
    "  We plan to implement the Reverse Mode of Auto differentiation. It accomplishes the same tasks as the Forward mode that we have already implemented but has different methods to accomplish this. The Reverse Mode has two passes, Forward and Reverse, and does not use the chain rule. Reverse Mode use the sensitivity of the function with respect to the independent variable and all intermediate variables in order to recover the gradient that was not calculated in each step because we did not use the chain rule. The Forward pass of Reverse Mode works to get the value of the function for that independent variable as well as the derivative with respect __ONLY__ that independent variable (no chain rule). The Reverse Pass then creates the chain rule that was not done in the Forward pass by traversing the path that was created in the Forward pass backwards. This traversal will work to store the sensitivity or the chain rule in each step. This back propagation works only with values and does not require us to evaluate the function further.\n",
    "   \n",
    "   To implement the Reverse Mode, we created a computational graph that stores all the values of the function based off of each node in the graph. It will calculate the gradient at the end when the AutoDiffR function is called. The Reverse mode will give the same responses as the Forward mode but it was just created differently under the hood. The following cells will show you how to use this portion of the package, and it will be quite similar to how the Forward Mode was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dc159e4-668b-45ee-9b68-afbd61ef27a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cs107_creativename.Reverse import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b896595a-57e8-468b-9e66-b1483bb7693a",
   "metadata": {},
   "source": [
    "#### Instantiate an object\n",
    "This will make the x variable a Reverse.Node class variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c35fb64-1428-416c-bdae-696ef9cbdc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'cs107_creativename.Reverse.Node'>\n"
     ]
    }
   ],
   "source": [
    "x = Node(3)\n",
    "print(type(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f64f3d-18a4-4374-9d43-fbea7d971fbf",
   "metadata": {},
   "source": [
    "#### Using the Reverse class with one dimension (i.e. having one variable x)\n",
    "Once we instantiate x, we can use it in function that we can then store the value of the function. \\\n",
    "**Note** we are not storing the value of the derivative here as we do in the Forward Mode. This is because of how we accomplish the Reverse Mode. You will be able to get the final gradient at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20d1f03a-7d97-44bd-b189-af339217e400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8414709848078967\n"
     ]
    }
   ],
   "source": [
    "x = Node(1)\n",
    "f = sin(x)+x +x**2\n",
    "print(f.val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab10da8-e3a4-4cd9-ba2a-3fd165670506",
   "metadata": {},
   "source": [
    "#### Using the Reverse class with Two dimensions (i.e having two variables x and y)\n",
    "The previous example can be expanded to instantiate multiple variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53795ad8-6b99-46ee-af74-7435ed419db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.09070257317431829\n"
     ]
    }
   ],
   "source": [
    "x = Node(1)\n",
    "y = Node(2)\n",
    "f = sin(x*y) + x**y - 2*x\n",
    "print(f.val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e9c65e-1777-4d69-91c8-1d96320b5173",
   "metadata": {},
   "source": [
    "#### Calculating gradient, note that the sin function is the ones we define\n",
    "How to fill out the AutoDiffR function: AutoDiffR(**function, values for variables in list format**).\n",
    "It returns the gradient, which includes the partial for each variable at the specified value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c19453a-6a3d-4bc7-b08b-7d2b1702c765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.5403023058681398, 27, 1]\n"
     ]
    }
   ],
   "source": [
    "def f(x, y, z):\n",
    "    return sin(x) + y**3 + x*z\n",
    "print(AutoDiffR(f,[1,3,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e541f0d-2f0f-4f70-bc23-3b0959a607d0",
   "metadata": {},
   "source": [
    "#### Can also use lambda functions \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed5655a0-2ce9-4f52-8ce5-6e7ff8bf1617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.5403023058681398, 27, 1]\n"
     ]
    }
   ],
   "source": [
    "func = lambda x,y,z: sin(x) + y**3 + x*z\n",
    "print(AutoDiffR(func,[1,3,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c29a17a-a852-4b97-a539-7a5f44c0091a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
